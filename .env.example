# LLM model and token limits
OPENAI_MODEL=gpt-4o-mini

# Per-stage output length controls (tokens)
# If unset or invalid, sensible defaults are used.
GW_MAX_TOKENS_DIALOG=120
GW_MAX_TOKENS_PRE_DRAFT=1200
GW_MAX_TOKENS_CHECK=1600
GW_MAX_TOKENS_SUGGESTIONS=800
GW_MAX_TOKENS_DRAFT=2200
GW_MAX_TOKENS_STORY_SO_FAR=1200
GW_MAX_TOKENS_STORY_RELATIVE=1400

# Dialog context lines default when not specified per call/template
GW_DIALOG_CONTEXT_LINES=8

# Iteration behavior
GW_MAX_ITERATIONS=2
GW_SKIP_SUMMARIES=0

# Optional per-stage model overrides
# If unset, OPENAI_MODEL is used.
GW_MODEL_DIALOG=
GW_MODEL_PRE_DRAFT=
GW_MODEL_CHECK=
GW_MODEL_SUGGESTIONS=
GW_MODEL_DRAFT=
GW_MODEL_STORY_SO_FAR=
GW_MODEL_STORY_RELATIVE=

# Optional per-stage temperature overrides
# Dialog prioritizes character/template temperature hints; these serve as fallbacks.
GW_TEMP_DIALOG=0.3
GW_TEMP_PRE_DRAFT=0.2
GW_TEMP_CHECK=0.0
GW_TEMP_SUGGESTIONS=0.0
GW_TEMP_DRAFT=0.2
GW_TEMP_STORY_SO_FAR=0.2
GW_TEMP_STORY_RELATIVE=0.2
# Copy to .env and fill in values
# Used in upcoming LLM integration (TODO task 4)
OPENAI_API_KEY=
# Optional: model and timeouts can be added in future tasks
# OPENAI_MODEL=gpt-4o
# REQUEST_TIMEOUT_SECONDS=60
